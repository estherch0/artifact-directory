Title:
Efficient Common Sense in Large Language Models via Knowledge Graph Compression

Abstract:
Commonsense knowledge is foundational for reasoning and decision making tasks, yet large language models (LLMs) can struggle with implicit knowledge, and are prone to hallucinating non-factual information. Knowledge graphs provide a rich source of structured commonsense relationships that can be used to inject factual knowledge directly into LLMs. However, the size and complexity of these types of graph data makes it difficult to simply feed it into an LLM to give it better intuition. In fact, feeding all this information into LLMs can overwhelm them, leading to confusion and poor performance. To solve this problem, we seek to develop a solution for compressing knowledge graphs into more manageable sizes using graph neural networks (GNNs). The graph compression method will selectively retain relevant common sense relationships while discarding unnecessary information. We hypothesize that efficiently compressing knowledge graphs will enable LLMs to effectively learn what is essential for their tasks, improving their ability to integrate commonsense knowledge for tasks such as semantic reasoning and abductive inference. Previous work by Hwang et al. (2023) has explored graph compression using relational graph convolutional networks (RGCNs). While RGCNs are effective for graph-based tasks, they often struggle with scalability and understanding long-range dependencies and global graph context. Our approach uses a transformer-based architecture to enhance scalability and preserve essential long range relationships, maintaining key knowledge while ensuring diversity in generated outputs. Using datasets ComVE and Î±NLG, we benchmark the effectiveness of our method with other work conducted by Hwang et al. (2023). By bridging the gap between large-scale graph-based knowledge and LLMs, our work contributes to more effective and context aware commonsense reasoning in NLP applications.
